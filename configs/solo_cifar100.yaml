defaults:
  - _self_

hydra:
  output_subdir: null
  run:
    dir: .
name: "All4One-cifar10" # change here for cifar10
method: "all4one"
dataset: "cifar100"
dataset_dir: "/buckets/ml/datasets"
representations_dir: "/buckets/ml/representations"
checkpoints_dir: "/buckets/ml/checkpoints"
overwrite : False
auto_augment: False
backbone:
  name: "resnet18"
  kwargs: {}
method_kwargs:
  clip_grad: 0
  freeze_last_layer: True
  norm_last_layer: True
  num_prototypes: 4096
  output_dim: 256
  pred_hidden_dim: 4096
  proj_hidden_dim: 2048
  proj_output_dim: 256
  queue_size: 98304
  temperature: 0.2
auto_resume:
  enabled: false
  max_hours: 36
data:
  dataset: cifar100
  format: image_folder
  fraction: -1
  no_labels: false
  num_classes: 100
  num_large_crops: 2
  num_small_crops: 0
  num_workers: 4
debug_augmentations: false
knn_eval:
  distance_func: euclidean
  enabled: false
  k: 20
momentum:
  base_tau: 0.99
  classifier: false
  final_tau: 1.0
num_nodes: 1
optimizer:
  batch_size: 250
  classifier_lr: 0.1
  exclude_bias_n_norm_wd: false
  kwargs:
    clip_lr: true
    eta: 0.02
    exclude_bias_n_norm: true
    momentum: 0.9
  lr: 1.0
  name: lars
  weight_decay: 1.0e-05
performance:
  disable_channel_last: false
precision: 16-mixed
resume_from_checkpoint: null
scheduler:
  interval: step
  lr_decay_steps: null
  min_lr: 0.0
  name: warmup_cosine
  warmup_epochs: 10
  warmup_start_lr: 3.0e-05
seed: 5
sync_batchnorm: true
wandb:
  enabled: true
  entity: null
  offline: false
  project: solo-learn
wandb_run_id: 8okpafh8
max_epochs: 1000
devices: [0]
accelerator: "gpu"
strategy: "ddp"
accumulate_grad_batches: True
